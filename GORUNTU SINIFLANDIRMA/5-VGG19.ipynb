{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYjUeGUmy_Oi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import time\n",
        "import cv2 as cv2\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "import os\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "from PIL import Image\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from IPython.core.display import display, HTML"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fpath=r'/content/drive/MyDrive/UZUM YAPRAK VERISETI/Ak/Ak (10).png'\n",
        "img=plt.imread(fpath)\n",
        "print (img.shape)\n",
        "imshow(img)"
      ],
      "metadata": {
        "id": "IIodGExiz_Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdir=r'/content/drive/MyDrive/UZUM YAPRAK VERISETI'\n",
        "working_dir=r'./'\n",
        "s_dir=os.path.join(working_dir, 'UZUM YAPRAK VERISETI')\n",
        "if os.path.isdir(s_dir):\n",
        "    shutil.rmtree(s_dir)\n",
        "shutil.copytree(sdir, s_dir)\n",
        "sdir = s_dir\n",
        "classlist=os.listdir(sdir)    \n",
        "filepaths=[]\n",
        "labels=[]    \n",
        "for sinif in classlist:\n",
        "    if sinif != 'nonsegmentedimage':\n",
        "        classpath=os.path.join(sdir,sinif)\n",
        "        if os.path.isdir(classpath):\n",
        "            flist=os.listdir(classpath)        \n",
        "            for f in flist:\n",
        "                fpath=os.path.join(classpath,f)        \n",
        "                filepaths.append(fpath)\n",
        "                labels.append(sinif)\n",
        "Fseries=pd.Series(filepaths, name='filepaths')\n",
        "Lseries=pd.Series(labels, name='labels')    \n",
        "df=pd.concat([Fseries, Lseries], axis=1)    \n",
        "print('df length: ', len(df))\n",
        "print (df['labels'].value_counts())"
      ],
      "metadata": {
        "id": "KxGvrKUT0BJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split=.9\n",
        "test_split=.1\n",
        "train_df, test_df=train_test_split(df, train_size=train_split, shuffle=True, random_state=123)\n",
        "print ('train_df length: ', len(train_df),'  test_df length: ', len(test_df))\n",
        "\n",
        "working_dir=r'./'\n",
        "test_dir=os.path.join(working_dir, 'test')\n",
        "if os.path.isdir(test_dir):\n",
        "    shutil.rmtree(test_dir)\n",
        "os.mkdir(test_dir)\n",
        "for label in df['labels'].unique():\n",
        "    dir_path=os.path.join(test_dir,label)    \n",
        "    os.mkdir(dir_path)\n",
        "print(os.listdir(test_dir))\n",
        "\n",
        "for f in test_df['filepaths']:\n",
        "    shutil.move(f,test_dir + f[32:])\n",
        "    test_df = test_df.replace(to_replace = f, value = test_dir + f[32:])"
      ],
      "metadata": {
        "id": "0iTDCGqM0EQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepaths=[]\n",
        "labels=[]    \n",
        "for sinif in classlist:\n",
        "    if sinif != 'nonsegmentedimage':\n",
        "        classpath=os.path.join(sdir,sinif)\n",
        "        if os.path.isdir(classpath):\n",
        "            flist=os.listdir(classpath)        \n",
        "            for f in flist:\n",
        "                fpath=os.path.join(classpath,f)        \n",
        "                filepaths.append(fpath)\n",
        "                labels.append(sinif)\n",
        "Fseries=pd.Series(filepaths, name='filepaths')\n",
        "Lseries=pd.Series(labels, name='labels')    \n",
        "df=pd.concat([Fseries, Lseries], axis=1)    \n",
        "print('df length: ', len(df))\n",
        "print (df['labels'].value_counts())"
      ],
      "metadata": {
        "id": "Fku4gqnc0HMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split=.9\n",
        "valid_split=.1\n",
        "train_df, valid_df=train_test_split(train_df, train_size=train_split, shuffle=True, random_state=123)\n",
        "print ('train_df length: ', len(train_df),'  test_df length: ', len(test_df), '  valid_df length: ', len(valid_df))"
      ],
      "metadata": {
        "id": "fxP9Sqi20BMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "height=66\n",
        "width=66\n",
        "channels=3\n",
        "batch_size=40\n",
        "img_shape=(height, width, channels)\n",
        "img_size=(height, width)\n",
        "length=len(test_df)\n",
        "test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  \n",
        "test_steps=int(length/test_batch_size)\n",
        "print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps)\n",
        "train_gen=ImageDataGenerator().flow_from_dataframe( train_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)\n",
        "test_gen=ImageDataGenerator().flow_from_dataframe( test_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=False, batch_size=test_batch_size)\n",
        "valid_gen=ImageDataGenerator().flow_from_dataframe( valid_df, x_col='filepaths', y_col='labels', target_size=img_size, class_mode='categorical',\n",
        "                                    color_mode='rgb', shuffle=True, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "Xkto9HXv0BPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_in_color(txt_msg,fore_tupple,back_tupple,):\n",
        "    rf,gf,bf=fore_tupple\n",
        "    rb,gb,bb=back_tupple\n",
        "    msg='{0}' + txt_msg\n",
        "    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n",
        "    print(msg .format(mat), flush=True)\n",
        "    print('\\33[0m', flush=True)\n",
        "    return"
      ],
      "metadata": {
        "id": "BsHk4x9b0BRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='VGG19'\n",
        "basemodel = tf.keras.applications.DenseNet121(weights = \"imagenet\",input_shape = img_shape,include_top=False,pooling='max')\n",
        "x=basemodel.output\n",
        "x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
        "x = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n",
        "                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\n",
        "x=Dropout(rate=.45, seed=123)(x)        \n",
        "output = Dense(5, activation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs = basemodel.inputs, outputs = output)\n",
        "model.compile(Adamax(lr=.001), loss='categorical_crossentropy', metrics=['accuracy']) \n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7fuJAcLT0RBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LRA(keras.callbacks.Callback):\n",
        "    reset=False\n",
        "    count=0\n",
        "    stop_count=0\n",
        "    \n",
        "    def __init__(self,model, patience,stop_patience, threshold, factor, dwell, model_name, freeze,batches, initial_epoch,epochs, ask_epoch):\n",
        "        super(LRA, self).__init__()\n",
        "        self.epochs=epochs\n",
        "        self.ask_epoch=ask_epoch\n",
        "        self.model=model\n",
        "        self.patience=patience \n",
        "        self.stop_patience=stop_patience\n",
        "        self.threshold=threshold \n",
        "        self.factor=factor \n",
        "        self.dwell=dwell\n",
        "        self.lr=float(tf.keras.backend.get_value(model.optimizer.lr)) \n",
        "        self.highest_tracc=0.0 \n",
        "        self.lowest_vloss=np.inf \n",
        "        self.initial_epoch=initial_epoch \n",
        "        self.batches=batches\n",
        "        best_weights=self.model.get_weights()         \n",
        "        msg=' '\n",
        "        if freeze==True:\n",
        "            msgs=f' Starting training using  base model { model_name} with weights frozen to imagenet weights initializing LRA callback'\n",
        "        else:\n",
        "            msgs=f' Starting training using base model { model_name} training all layers '            \n",
        "        print_in_color (msgs, (244, 252, 3), (55,65,80)) \n",
        "    def on_train_begin(self, logs=None):\n",
        "        msg='{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:^8s}'.format('Epoch', 'Loss', 'Accuracy',\n",
        "                                                                                              'V_loss','V_acc', 'LR', 'Next LR', 'Monitor', 'Duration')\n",
        "        print_in_color(msg, (244,252,3), (55,65,80)) \n",
        "    def on_train_end(self, logs=None):\n",
        "        model.set_weights(LRA.best_weights)\n",
        "        msg='Training is completed - model is set with weights for the epoch with the lowest loss'\n",
        "        print_in_color(msg, (0,255,0), (55,65,80)) \n",
        "        \n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        acc=logs.get('accuracy')* 100  \n",
        "        loss=logs.get('loss')\n",
        "        msg='{0:20s}processing batch {1:4s} of {2:5s} accuracy= {3:8.3f}  loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
        "        print(msg, '\\r', end='') \n",
        "        \n",
        "        \n",
        "    def on_epoch_begin(self,epoch, logs=None):\n",
        "        self.now= time.time()\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs=None):  \n",
        "        later=time.time()\n",
        "        duration=later-self.now \n",
        "        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) \n",
        "        current_lr=lr\n",
        "        v_loss=logs.get('val_loss')  \n",
        "        acc=logs.get('accuracy')  \n",
        "        v_acc=logs.get('val_accuracy')\n",
        "        loss=logs.get('loss')\n",
        "        if acc < self.threshold: \n",
        "            monitor='accuracy'\n",
        "            if acc>self.highest_tracc:                 \n",
        "                self.highest_tracc=acc \n",
        "                LRA.best_weights=self.model.get_weights()\n",
        "                self.count=0\n",
        "                self.stop_count=0 \n",
        "                if v_loss<self.lowest_vloss:\n",
        "                    self.lowest_vloss=v_loss\n",
        "                color= (0,255,0)\n",
        "                self.lr=lr\n",
        "            else: \n",
        "                if self.count>=self.patience -1:\n",
        "                    color=(245, 170, 66)\n",
        "                    self.lr= lr* self.factor \n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) \n",
        "                    self.count=0 \n",
        "                    self.stop_count=self.stop_count + 1\n",
        "                    if self.dwell:\n",
        "                        self.model.set_weights(LRA.best_weights)                     \n",
        "                    else:\n",
        "                        if v_loss<self.lowest_vloss:\n",
        "                            self.lowest_vloss=v_loss                                    \n",
        "                else:\n",
        "                    self.count=self.count +1                    \n",
        "        else: \n",
        "            monitor='val_loss'\n",
        "            if v_loss< self.lowest_vloss: \n",
        "                self.lowest_vloss=v_loss           \n",
        "                LRA.best_weights=self.model.get_weights() \n",
        "                self.count=0 \n",
        "                self.stop_count=0  \n",
        "                color=(0,255,0)\n",
        "                self.lr=lr\n",
        "            else: \n",
        "                if self.count>=self.patience-1:\n",
        "                    color=(245, 170, 66)\n",
        "                    self.lr=self.lr * self.factor                \n",
        "                    self.stop_count=self.stop_count + 1 \n",
        "                    self.count=0 \n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, self.lr) \n",
        "                    if self.dwell:\n",
        "                        self.model.set_weights(LRA.best_weights) \n",
        "                else: \n",
        "                    self.count =self.count +1               \n",
        "                if acc>self.highest_tracc:\n",
        "                    self.highest_tracc= acc\n",
        "        msg=f'{str(epoch+1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc*100:^9.3f}{v_loss:^9.5f}{v_acc*100:^9.3f}{current_lr:^9.5f}{self.lr:^9.5f}{monitor:^11s}{duration:^8.2f}'\n",
        "        print_in_color (msg,color, (55,65,80))\n",
        "        if self.stop_count> self.stop_patience - 1: \n",
        "            msg=f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
        "            print_in_color(msg, (0,255,255), (55,65,80))\n",
        "            self.model.stop_training = True\n",
        "        else: \n",
        "            if self.ask_epoch !=None:\n",
        "                if epoch + 1 >= self.ask_epoch:\n",
        "                    msg='enter H to halt training or an integer for number of epochs to run then ask again'\n",
        "                    print_in_color(msg, (0,255,255), (55,65,80))\n",
        "                    ans=input('')\n",
        "                    if ans=='H' or ans=='h':\n",
        "                        msg=f'training has been halted at epoch {epoch + 1} due to user input'\n",
        "                        print_in_color(msg, (0,255,255), (55,65,80))\n",
        "                        self.model.stop_training = True\n",
        "                    else:\n",
        "                        ans=int(ans)\n",
        "                        self.ask_epoch +=ans"
      ],
      "metadata": {
        "id": "ZhBYsHyU0T4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs =40\n",
        "patience= 1 \n",
        "stop_patience =3 \n",
        "threshold=.9 \n",
        "factor=.5 \n",
        "dwell=True \n",
        "freeze=False\n",
        "ask_epoch=10 \n",
        "batches= int(len(train_df.labels)/batch_size)\n",
        "callbacks=[LRA(model=model,patience=patience,stop_patience=stop_patience, threshold=threshold,\n",
        "                   factor=factor,dwell=dwell, model_name=model_name, freeze=freeze, batches=batches,initial_epoch=0,epochs=epochs, ask_epoch=ask_epoch )]\n",
        "\n",
        "history=model.fit(x=train_gen,  epochs=epochs, verbose=0, callbacks=callbacks,  validation_data=valid_gen,\n",
        "               validation_steps=None,  shuffle=False,  initial_epoch=0)"
      ],
      "metadata": {
        "id": "S6LXwBPg0T6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tr_plot(tr_data, start_epoch):\n",
        "    tacc=tr_data.history['accuracy']\n",
        "    tloss=tr_data.history['loss']\n",
        "    vacc=tr_data.history['val_accuracy']\n",
        "    vloss=tr_data.history['val_loss']\n",
        "    Epoch_count=len(tacc)+ start_epoch\n",
        "    Epochs=[]\n",
        "    for i in range (start_epoch ,Epoch_count):\n",
        "        Epochs.append(i+1)   \n",
        "    index_loss=np.argmin(vloss)\n",
        "    val_lowest=vloss[index_loss]\n",
        "    index_acc=np.argmax(vacc)\n",
        "    acc_highest=vacc[index_acc]\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    sc_label='best epoch= '+ str(index_loss+1 +start_epoch)\n",
        "    vc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\n",
        "    fig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\n",
        "    axes[0].plot(Epochs,tloss, 'r', label='Training loss')\n",
        "    axes[0].plot(Epochs,vloss,'g',label='Validation loss' )\n",
        "    axes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\n",
        "    axes[0].set_title('Training and Validation Loss')\n",
        "    axes[0].set_xlabel('Epochs')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[1].plot (Epochs,tacc,'r',label= 'Training Accuracy')\n",
        "    axes[1].plot (Epochs,vacc,'g',label= 'Validation Accuracy')\n",
        "    axes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\n",
        "    axes[1].set_title('Training and Validation Accuracy')\n",
        "    axes[1].set_xlabel('Epochs')\n",
        "    axes[1].set_ylabel('Accuracy')\n",
        "    axes[1].legend()\n",
        "    plt.tight_layout\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Fuqyw0h30cD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_info( test_gen, preds, print_code, save_dir, subject ):\n",
        "    class_dict=test_gen.class_indices\n",
        "    labels= test_gen.labels\n",
        "    file_names= test_gen.filenames \n",
        "    error_list=[]\n",
        "    true_class=[]\n",
        "    pred_class=[]\n",
        "    prob_list=[]\n",
        "    new_dict={}\n",
        "    error_indices=[]\n",
        "    y_pred=[]\n",
        "    for key,value in class_dict.items():\n",
        "        new_dict[value]=key             \n",
        "    classes=list(new_dict.values())   \n",
        "    dict_as_text=str(new_dict)\n",
        "    dict_name= subject + '-' +str(len(classes)) +'.txt'  \n",
        "    dict_path=os.path.join(save_dir,dict_name)    \n",
        "    with open(dict_path, 'w') as x_file:\n",
        "        x_file.write(dict_as_text)    \n",
        "    errors=0      \n",
        "    for i, p in enumerate(preds):\n",
        "        pred_index=np.argmax(p)        \n",
        "        true_index=labels[i] \n",
        "        if pred_index != true_index: \n",
        "            error_list.append(file_names[i])\n",
        "            true_class.append(new_dict[true_index])\n",
        "            pred_class.append(new_dict[pred_index])\n",
        "            prob_list.append(p[pred_index])\n",
        "            error_indices.append(true_index)            \n",
        "            errors=errors + 1\n",
        "        y_pred.append(pred_index)    \n",
        "    if print_code !=0:\n",
        "        if errors>0:\n",
        "            if print_code>errors:\n",
        "                r=errors\n",
        "            else:\n",
        "                r=print_code           \n",
        "            msg='{0:^28s}{1:^28s}{2:^28s}{3:^16s}'.format('Filename', 'Predicted Class' , 'True Class', 'Probability')\n",
        "            print_in_color(msg, (0,255,0),(55,65,80))\n",
        "            for i in range(r):                \n",
        "                split1=os.path.split(error_list[i])                \n",
        "                split2=os.path.split(split1[0])                \n",
        "                fname=split2[1] + '/' + split1[1]\n",
        "                msg='{0:^28s}{1:^28s}{2:^28s}{3:4s}{4:^6.4f}'.format(fname, pred_class[i],true_class[i], ' ', prob_list[i])\n",
        "                print_in_color(msg, (255,255,255), (55,65,60))            \n",
        "        else:\n",
        "            msg='With accuracy of 100 % there are no errors to print'\n",
        "            print_in_color(msg, (0,255,0),(55,65,80))\n",
        "    if errors>0:\n",
        "        plot_bar=[]\n",
        "        plot_class=[]\n",
        "        for  key, value in new_dict.items():        \n",
        "            count=error_indices.count(key) \n",
        "            if count!=0:\n",
        "                plot_bar.append(count) \n",
        "                plot_class.append(value) \n",
        "        fig=plt.figure()\n",
        "        fig.set_figheight(len(plot_class)/3)\n",
        "        fig.set_figwidth(10)\n",
        "        plt.style.use('fivethirtyeight')\n",
        "        for i in range(0, len(plot_class)):\n",
        "            c=plot_class[i]\n",
        "            x=plot_bar[i]\n",
        "            plt.barh(c, x, )\n",
        "            plt.title( ' Errors by Class on Test Set')\n",
        "    y_true= np.array(labels)        \n",
        "    y_pred=np.array(y_pred)\n",
        "    if len(classes)<= 30:\n",
        "        cm = confusion_matrix(y_true, y_pred )        \n",
        "        length=len(classes)\n",
        "        if length<8:\n",
        "            fig_width=8\n",
        "            fig_height=8\n",
        "        else:\n",
        "            fig_width= int(length * .5)\n",
        "            fig_height= int(length * .5)\n",
        "        plt.figure(figsize=(fig_width, fig_height))\n",
        "        sns.heatmap(cm, annot=True, vmin=0, fmt='g', cmap='Blues', cbar=False)       \n",
        "        plt.xticks(np.arange(length)+.5, classes, rotation= 90)\n",
        "        plt.yticks(np.arange(length)+.5, classes, rotation=0)\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"Actual\")\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "        plt.show()\n",
        "    clr = classification_report(y_true, y_pred, target_names=classes)\n",
        "    print(\"Classification Report:\\n----------------------\\n\", clr)"
      ],
      "metadata": {
        "id": "LKTtyh0W0cL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_plot(history,0)\n",
        "save_dir=r'./'\n",
        "subject='plants'\n",
        "acc=model.evaluate( test_gen, batch_size=test_batch_size, verbose=1, steps=test_steps, return_dict=False)[1]*100\n",
        "msg=f'accuracy on the test set is {acc:5.2f} %'\n",
        "print_in_color(msg, (0,255,0),(55,65,80))\n",
        "save_id=str (model_name +  '-' + subject +'-'+ str(acc)[:str(acc).rfind('.')+3] + '.h5')\n",
        "save_loc=os.path.join(save_dir, save_id)\n",
        "model.save(save_loc)"
      ],
      "metadata": {
        "id": "dZbRK8yO0lZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_code=0\n",
        "preds=model.predict(test_gen) \n",
        "print_info( test_gen, preds, print_code, save_dir, subject )"
      ],
      "metadata": {
        "id": "OQJX_fVK0oWq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}